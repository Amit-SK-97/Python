{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaa59983",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e042a75",
   "metadata": {},
   "source": [
    "Objective:\n",
    "I can do a text summary that will let me undertsand the important words used in the dataset.\n",
    "I can first understand about the dataset.\n",
    "Clean the tweets by removing 'Mentions','Special Characters','HTML','UTF-8-bom' if any.\n",
    "After which techniques like bag of words that uses stemming and lemmetization can be done to finally get the text summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2104ceb7",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be64cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "cols = ['sentiment','id','date','query_string','user','text']\n",
    "df = pd.read_csv(\"N:/Semester 3/Text Mining/trainingandtestdata/testdata.manual.2009.06.14.csv\",header=None, names=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb7ed5f",
   "metadata": {},
   "source": [
    "3.a) Display the Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed0359a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query_string</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>tpryan</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>vcu451</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>chadfu</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>SIX15</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
       "      <td>kindle2</td>\n",
       "      <td>yamarama</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment  id                          date query_string      user  \\\n",
       "0          4   3  Mon May 11 03:17:40 UTC 2009      kindle2    tpryan   \n",
       "1          4   4  Mon May 11 03:18:03 UTC 2009      kindle2    vcu451   \n",
       "2          4   5  Mon May 11 03:18:54 UTC 2009      kindle2    chadfu   \n",
       "3          4   6  Mon May 11 03:19:04 UTC 2009      kindle2     SIX15   \n",
       "4          4   7  Mon May 11 03:21:41 UTC 2009      kindle2  yamarama   \n",
       "\n",
       "                                                text  \n",
       "0  @stellargirl I loooooooovvvvvveee my Kindle2. ...  \n",
       "1  Reading my kindle2...  Love it... Lee childs i...  \n",
       "2  Ok, first assesment of the #kindle2 ...it fuck...  \n",
       "3  @kenburbary You'll love your Kindle2. I've had...  \n",
       "4  @mikefish  Fair enough. But i have the Kindle2...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eff1c43",
   "metadata": {},
   "source": [
    "3.b) Dimension of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67f48a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(498, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed4adf8",
   "metadata": {},
   "source": [
    "It has 498 observations and 6 Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e1e272",
   "metadata": {},
   "source": [
    "3.d) Datatypes of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b86b4a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment        int64\n",
      "id               int64\n",
      "date            object\n",
      "query_string    object\n",
      "user            object\n",
      "text            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43e1698",
   "metadata": {},
   "source": [
    "4. Filter the columns and get only the columns used for anlysis - PreProcess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edd330e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['id','date','query_string','user'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89f1c7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          4  @stellargirl I loooooooovvvvvveee my Kindle2. ...\n",
       "1          4  Reading my kindle2...  Love it... Lee childs i...\n",
       "2          4  Ok, first assesment of the #kindle2 ...it fuck...\n",
       "3          4  @kenburbary You'll love your Kindle2. I've had...\n",
       "4          4  @mikefish  Fair enough. But i have the Kindle2..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f132ab64",
   "metadata": {},
   "source": [
    "Unwamted coloumns are removed. Now we clean the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dadc214",
   "metadata": {},
   "source": [
    "Checking for tweets for more than 280 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b08c0afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pre_clean_len'] = [len(t) for t in df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dd612b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>pre_clean_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sentiment, text, pre_clean_len]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.pre_clean_len > 280].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774768e6",
   "metadata": {},
   "source": [
    "We dont have any. So all the texts are legitimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6067390d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\amitniki\\anaconda3\\lib\\site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\amitniki\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\amitniki\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.20.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\amitniki\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\amitniki\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=612ca16d2dc10bee4080d0efcffd502bcf07aadb8375a3bd78cf4627be7090d7\n",
      "  Stored in directory: c:\\users\\amitniki\\appdata\\local\\pip\\cache\\wheels\\22\\0b\\40\\fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a153ab",
   "metadata": {},
   "source": [
    "Trying to remove unnecessary symbols like '@', removing htmls, and others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff095c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i loooooooovvvvvveee my kindle not that the dx is cool but the is fantastic in its own right',\n",
       " 'reading my kindle love it lee childs is good read',\n",
       " 'ok first assesment of the kindle it fucking rocks',\n",
       " 'you ll love your kindle i ve had mine for a few months and never looked back the new big one is huge no need for remorse',\n",
       " 'fair enough but i have the kindle and i think it s perfect',\n",
       " 'no it is too big i m quite happy with the kindle',\n",
       " 'fuck this economy i hate aig and their non loan given asses',\n",
       " 'jquery is my new best friend',\n",
       " 'loves twitter',\n",
       " 'how can you not love obama he makes jokes about himself',\n",
       " 'check this video out president obama at the white house correspondents dinner',\n",
       " 'i firmly believe that obama pelosi have zero desire to be civil it s a charade and a slogan but they want to destroy conservatism',\n",
       " 'house correspondents dinner was last night whoopi barbara sherri went obama got a standing ovation',\n",
       " 'watchin espn jus seen this new nike commerical with a puppet lebron sh t was hilarious lmao',\n",
       " 'dear nike stop with the flywire that shit is a waste of science and ugly love',\n",
       " 'lebron best athlete of our generation if not all time basketball related i don t want to get into inter sport debates about',\n",
       " 'i was talking to this guy last night and he was telling me that he is a die hard spurs fan he also told me that he hates lebron james',\n",
       " 'i love lebron',\n",
       " 'lebron is a beast but i m still cheering the a til the end',\n",
       " 'lebron is the boss',\n",
       " 'lebron is a hometown hero to me lol i love the lakers but let s go cavs lol',\n",
       " 'lebron and zydrunas are such an awesome duo',\n",
       " 'lebron is a beast nobody in the nba comes even close',\n",
       " 'downloading apps for my iphone so much fun there literally is an app for just about anything',\n",
       " 'good news just had a call from the visa office saying everything is fine what a relief i am sick of scams out there stealing',\n",
       " 'awesome come back from via',\n",
       " 'in montreal for a long weekend of r r much needed',\n",
       " 'booz allen hamilton has a bad ass homegrown social collaboration platform way cool ttiv',\n",
       " 'mluc customer innovation award winner booz allen hamilton',\n",
       " 'i current use the nikon d and love it but not as much as the canon d d i chose the d for the video feature my mistake',\n",
       " 'need suggestions for a good ir filter for my canon d got some pls dm',\n",
       " 'i just checked my google for my business blip shows up as the second entry huh is that a good or ba emhv',\n",
       " 'google is always a good place to look should ve mentioned i worked on the mustang w my dad',\n",
       " 'played with an android google phone the slide out screen scares me i would break that fucker so fast still prefer my iphone',\n",
       " 'us planning to resume the military tribunals at guantanamo bay only this time those on trial will be aig execs and chrysler debt holders',\n",
       " 'omg so bored my tattoooos are so itchy help aha',\n",
       " 'i m itchy and miserable',\n",
       " 'no i m not itchy for now maybe later lol',\n",
       " 'rt i love the nerdy stanford human biology videos makes me miss school',\n",
       " 'has been a bit crazy with steep learning curve but lyx is really good for long docs for anything shorter it would be insane',\n",
       " 'i m listening to p y t by danny gokey aww he s so amazing i him so much',\n",
       " 'is going to sleep then on a bike ride',\n",
       " 'cant sleep my tooth is aching',\n",
       " 'blah blah blah same old same old no plans today going back to sleep i guess',\n",
       " 'glad i didnt do bay to breakers today it s freaking degrees in san francisco wtf',\n",
       " 'is in san francisco at bay to breakers',\n",
       " 'just landed at san francisco',\n",
       " 'san francisco today any suggestions',\n",
       " 'obama administration must stop bonuses to aig ponzi schemers',\n",
       " 'started to think that citi is in really deep s t are they gonna survive the turmoil or are they gonna be the next aig',\n",
       " 'shaunwoo hate n on aig',\n",
       " 'you will not regret going to see star trek it was awesome',\n",
       " 'on my way to see star trek the esquire',\n",
       " 'going to see star trek soon with my dad',\n",
       " 'annoying new trend on the internets people picking apart michael lewis and malcolm gladwell nobody wants to read that',\n",
       " 'bill simmons in conversation with malcolm gladwell',\n",
       " 'highly recommend by malcolm gladwell',\n",
       " 'blink by malcolm gladwell amazing book and the tipping point',\n",
       " 'malcolm gladwell might be my new man crush',\n",
       " 'omg the commercials alone on espn are going to drive me nuts',\n",
       " 'playing with twitter api sounds fun may need to take a class or find a new friend who like to generate results with api code',\n",
       " 'playing with curl and the twitter api',\n",
       " 'hello twitter api',\n",
       " 'playing with java and the twitter api',\n",
       " 'because the twitter api is slow and most client s aren t good',\n",
       " 'yahoo answers can be a butt sometimes',\n",
       " 'is scrapbooking with nic d',\n",
       " 'rt five things wolfram alpha does better and vastly different than google',\n",
       " 'just changed my default pic to a nike basketball cause bball is awesome',\n",
       " 'nike owns nba playoffs ads w lebron kobe carmelo adidas billups howard marketing branding',\n",
       " 'next time i ll call myself nike',\n",
       " 'new blog post nike sb dunk low premium white gum',\n",
       " 'rt was just told that nike layoffs started today',\n",
       " 'back when i worked for nike we had one fav word just do it',\n",
       " 'by the way i m totally inspired by this freaky nike commercial',\n",
       " 'giving weka an app engine interface using the bird strike data for the tests the logo is a given',\n",
       " 'brand new canon eos d mp dslr camera canon mm is lens web technology thread brand new canon eos',\n",
       " 'class the d is supposed to come today',\n",
       " 'needs someone to explain lambda calculus to him',\n",
       " 'took the graduate field exam for computer science today nothing makes you feel like more of an idiot than lambda calculus',\n",
       " 'shout outs to all east palo alto for being in the buildin karizmakaze cal gta also thanks to profits of doom universal hempz cracka',\n",
       " 'yeahhhhhhhhh i wouldn t really have lived in east palo alto if i could have avoided it i guess it s only for the summer',\n",
       " 'great stanford course thanks for making it available to the public really helpful and informative for starting off',\n",
       " 'nvidia names stanford s bill dally chief scientist vp of research',\n",
       " 'new blog post harvard versus stanford who wins',\n",
       " 'work til pm lets go lakers',\n",
       " 'damn you north korea',\n",
       " 'can we just go ahead and blow north korea off the map already',\n",
       " 'north korea please cease this douchebaggery china doesn t even like you anymore',\n",
       " 'why the hell is pelosi in freakin china and on whose dime',\n",
       " 'are you burning more cash than chrysler and gm stop the financial tsunami where bailout means taking a handout',\n",
       " 'insects have infected my spinach plant',\n",
       " 'wish i could catch every mosquito in the world n burn em slowly they been bitin the shit outta me day mosquitos are the assholes of insects',\n",
       " 'just got back from church and i totally hate insects',\n",
       " 'just got mcdonalds goddam those eggs make me sick o yeah laker up date go lakers not much of an update well it s true so suck it',\n",
       " 'omgg i ohhdee want mcdonalds damn i wonder if its open lol',\n",
       " 'history exam studying ugh',\n",
       " 'i hate revision it s so boring i am totally unprepared for my exam tomorrow things are not looking good',\n",
       " 'higher physics exam tommorow not lookin forward to it much',\n",
       " 'it s a bank holiday yet i m only out of work now exam season sucks']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "def tweet_cleaner(text):\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    stripped = re.sub(combined_pat, '', souped)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "testing = df.text[:100]\n",
    "test_result = []\n",
    "for t in testing:\n",
    "    test_result.append(tweet_cleaner(t))\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec9f98fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13b4b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    " \n",
    "\n",
    "tf_idf_vec = TfidfVectorizer(use_idf=True,smooth_idf=False,ngram_range=(1,1),stop_words='english')\n",
    "\n",
    "tf_idf_data = tf_idf_vec.fit_transform(test_result)\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7bdefda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    aching  adidas  administration  ads  aha  ahead  aig  allen  alpha  alto  \\\n",
      "0      0.0     0.0             0.0  0.0  0.0    0.0  0.0    0.0    0.0   0.0   \n",
      "1      0.0     0.0             0.0  0.0  0.0    0.0  0.0    0.0    0.0   0.0   \n",
      "2      0.0     0.0             0.0  0.0  0.0    0.0  0.0    0.0    0.0   0.0   \n",
      "3      0.0     0.0             0.0  0.0  0.0    0.0  0.0    0.0    0.0   0.0   \n",
      "4      0.0     0.0             0.0  0.0  0.0    0.0  0.0    0.0    0.0   0.0   \n",
      "..     ...     ...             ...  ...  ...    ...  ...    ...    ...   ...   \n",
      "95     0.0     0.0             0.0  0.0  0.0    0.0  0.0    0.0    0.0   0.0   \n",
      "96     0.0     0.0             0.0  0.0  0.0    0.0  0.0    0.0    0.0   0.0   \n",
      "97     0.0     0.0             0.0  0.0  0.0    0.0  0.0    0.0    0.0   0.0   \n",
      "98     0.0     0.0             0.0  0.0  0.0    0.0  0.0    0.0    0.0   0.0   \n",
      "99     0.0     0.0             0.0  0.0  0.0    0.0  0.0    0.0    0.0   0.0   \n",
      "\n",
      "    ...      work  worked  world  wouldn  wtf  yahoo  yeah  yeahhhhhhhhh  \\\n",
      "0   ...  0.000000     0.0    0.0     0.0  0.0    0.0   0.0           0.0   \n",
      "1   ...  0.000000     0.0    0.0     0.0  0.0    0.0   0.0           0.0   \n",
      "2   ...  0.000000     0.0    0.0     0.0  0.0    0.0   0.0           0.0   \n",
      "3   ...  0.000000     0.0    0.0     0.0  0.0    0.0   0.0           0.0   \n",
      "4   ...  0.000000     0.0    0.0     0.0  0.0    0.0   0.0           0.0   \n",
      "..  ...       ...     ...    ...     ...  ...    ...   ...           ...   \n",
      "95  ...  0.000000     0.0    0.0     0.0  0.0    0.0   0.0           0.0   \n",
      "96  ...  0.000000     0.0    0.0     0.0  0.0    0.0   0.0           0.0   \n",
      "97  ...  0.000000     0.0    0.0     0.0  0.0    0.0   0.0           0.0   \n",
      "98  ...  0.000000     0.0    0.0     0.0  0.0    0.0   0.0           0.0   \n",
      "99  ...  0.381516     0.0    0.0     0.0  0.0    0.0   0.0           0.0   \n",
      "\n",
      "    zero  zydrunas  \n",
      "0    0.0       0.0  \n",
      "1    0.0       0.0  \n",
      "2    0.0       0.0  \n",
      "3    0.0       0.0  \n",
      "4    0.0       0.0  \n",
      "..   ...       ...  \n",
      "95   0.0       0.0  \n",
      "96   0.0       0.0  \n",
      "97   0.0       0.0  \n",
      "98   0.0       0.0  \n",
      "99   0.0       0.0  \n",
      "\n",
      "[100 rows x 503 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_dataframe=pd.DataFrame(tf_idf_data.toarray(),columns=tf_idf_vec.get_feature_names())\n",
    "print(tf_idf_dataframe)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7b0071",
   "metadata": {},
   "source": [
    "7. Commenting on the matrix values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ee9440",
   "metadata": {},
   "source": [
    "a matrix is created\n",
    "where the rows are tf_idf values with respect to a nonstop english word in a document.\n",
    "the coloums are non stop english words given in the entire file\n",
    "where, \n",
    "tf= number of times a word occured in a document / total number of words in a document\n",
    "idf = 1+ log(total number of documents in the dataset/total number of documents in which the nth word occur)\n",
    "TF-IDF = TF*IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dab73f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
